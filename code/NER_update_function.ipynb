{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25dd78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0f5dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle spaCy approprié pour la langue\n",
    "nlp = spacy.load(\"fr_core_news_sm\")  # Chargez le modèle français\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc08ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_graph(file) :\n",
    "    file_name = os.path.splitext(os.path.basename(file))[0] #extraction du nom du fichier\n",
    "    \n",
    "        # Lire le contenu du fichier .txt\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Augmenter la limite de longueur de texte\n",
    "    nlp.max_length = len(text) + 100  # Augmentez la limite de 100 caractères pour être sûr\n",
    "\n",
    "    # Analyse du texte avec spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialiser un dictionnaire pour stocker les distances entre les entités nommées\n",
    "    distance_dict = {}\n",
    "\n",
    "    # Extraction des entités nommées de type \"PERSON\"\n",
    "    #characters = [ent.text for ent in doc.ents if ent.label_ == \"PER\"]\n",
    "    characters = [ent.text for ent in doc.ents if ent.label_ == \"PER\" and not any(token.pos_ == \"PRON\" for token in ent)]\n",
    "\n",
    "\n",
    "    # Parcourir le document pour calculer les distances entre les entités nommées\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i].text in characters:\n",
    "            for j in range(i + 1, min(i + 26, len(doc))):\n",
    "                if doc[j].text in characters:\n",
    "                    # Calculer la distance entre les entités nommées\n",
    "                    distance = j - i - 1\n",
    "\n",
    "                    # Assurer que la distance ne dépasse pas 25\n",
    "                    distance = min(distance, 25)\n",
    "\n",
    "                    # Créer une clé unique pour la paire d'entités nommées\n",
    "                    key = tuple(sorted([doc[i].text, doc[j].text]))\n",
    "\n",
    "                    # Ajouter la distance au dictionnaire\n",
    "                    distance_dict[key] = distance\n",
    "\n",
    "    # Créer un graphe avec NetworkX\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Ajouter les nœuds (entités nommées)\n",
    "    G.add_nodes_from(characters)\n",
    "\n",
    "    # Ajouter les arêtes (relations) avec les distances comme attributs\n",
    "    for (char1, char2), distance in distance_dict.items():\n",
    "        G.add_edge(char1, char2, distance=distance)\n",
    "\n",
    "    # Enregistrer le graphe au format GEXF\n",
    "    nx.write_gexf(G, f\"../fig/{file_name}.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5f9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_e = \"../fomatage_utf8/Fondation_et_empire_sample.txt\"\n",
    "f_f = \"../fomatage_utf8/Fondation_foudroyée_sample.txt\"\n",
    "f_s = \"../fomatage_utf8/Fondation_sample.txt\"\n",
    "s_f = \"../fomatage_utf8/Seconde_Fondation_sample.txt\"\n",
    "t_f = \"../fomatage_utf8/Terre_et_Fondation_sample.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1bbf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "chap1 = \"../data/corpus_ASIMOV_leaderboard/corpus_asimov_leaderboard/les_cavernes_d_acier/chapter_1.txt.preprocessed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5cc277",
   "metadata": {},
   "outputs": [],
   "source": [
    "chap1 = \"../data/corpus_ASIMOV_leaderboard/corpus_asimov_leaderboard/prelude_a_fondation/chapter_1.txt.preprocessed\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d83d5bd",
   "metadata": {},
   "source": [
    "file_list = [f_e, f_f, f_s, s_f, t_f]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71595264",
   "metadata": {},
   "source": [
    "for f in file_list :\n",
    "    ner_graph(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b94b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_graph(chap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "392b2512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "import networkx as nx\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def ner_graph_lca(file):\n",
    "    file_name = os.path.splitext(os.path.basename(file))[0]  # extraction du nom du fichier\n",
    "\n",
    "    # Lire le contenu du fichier .txt\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Augmenter la limite de longueur de texte\n",
    "    nlp.max_length = len(text) + 100  # Augmentez la limite de 100 caractères pour être sûr\n",
    "\n",
    "    # Analyse du texte avec spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialiser une liste pour stocker les entités nommées de type \"PER\"\n",
    "    characters = [ent.text for ent in doc.ents if ent.label_ == \"PER\" and not any(token.pos_ == \"PRON\" for token in ent)]\n",
    "\n",
    "    # Initialiser un dictionnaire pour stocker les distances entre les entités nommées\n",
    "    distance_dict = {}\n",
    "\n",
    "    # Parcourir le document pour calculer les distances entre les entités nommées\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i].text in characters:\n",
    "            for j in range(i + 1, min(i + 26, len(doc))):\n",
    "                if doc[j].text in characters:\n",
    "                    # Calculer la distance entre les entités nommées\n",
    "                    distance = j - i - 1\n",
    "\n",
    "                    # Assurer que la distance ne dépasse pas 25\n",
    "                    distance = min(distance, 25)\n",
    "\n",
    "                    # Créer une clé unique pour la paire d'entités nommées\n",
    "                    key = tuple(sorted([doc[i].text, doc[j].text]))\n",
    "\n",
    "                    # Ajouter la distance au dictionnaire\n",
    "                    distance_dict[key] = distance\n",
    "\n",
    "    # Créer un graphe avec NetworkX\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Ajouter les nœuds (entités nommées)\n",
    "    G.add_nodes_from(characters)\n",
    "\n",
    "    # Ajouter les arêtes (relations) avec les distances comme attributs\n",
    "    for (char1, char2), distance in distance_dict.items():\n",
    "        G.add_edge(char1, char2, distance=distance)\n",
    "\n",
    "    # Enregistrer le graphe au format GEXF\n",
    "    nx.write_gexf(G, f\"../fig/{file_name}.gexf\")\n",
    "\n",
    "    # Enregistrer les entités nommées dans un fichier CSV\n",
    "    csv_file_path = f\"../fig/lca/{file_name}_PER.csv\"\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Person'])  # Header\n",
    "        csv_writer.writerows([[char] for char in characters])\n",
    "\n",
    "    return csv_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86df98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "import networkx as nx\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def ner_graph_paf(file):\n",
    "    file_name = os.path.splitext(os.path.basename(file))[0]  # extraction du nom du fichier\n",
    "\n",
    "    # Lire le contenu du fichier .txt\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Augmenter la limite de longueur de texte\n",
    "    nlp.max_length = len(text) + 100  # Augmentez la limite de 100 caractères pour être sûr\n",
    "\n",
    "    # Analyse du texte avec spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialiser une liste pour stocker les entités nommées de type \"PER\"\n",
    "    characters = [ent.text for ent in doc.ents if ent.label_ == \"PER\" and not any(token.pos_ == \"PRON\" for token in ent)]\n",
    "\n",
    "    # Initialiser un dictionnaire pour stocker les distances entre les entités nommées\n",
    "    distance_dict = {}\n",
    "\n",
    "    # Parcourir le document pour calculer les distances entre les entités nommées\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i].text in characters:\n",
    "            for j in range(i + 1, min(i + 26, len(doc))):\n",
    "                if doc[j].text in characters:\n",
    "                    # Calculer la distance entre les entités nommées\n",
    "                    distance = j - i - 1\n",
    "\n",
    "                    # Assurer que la distance ne dépasse pas 25\n",
    "                    distance = min(distance, 25)\n",
    "\n",
    "                    # Créer une clé unique pour la paire d'entités nommées\n",
    "                    key = tuple(sorted([doc[i].text, doc[j].text]))\n",
    "\n",
    "                    # Ajouter la distance au dictionnaire\n",
    "                    distance_dict[key] = distance\n",
    "\n",
    "    # Créer un graphe avec NetworkX\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Ajouter les nœuds (entités nommées)\n",
    "    G.add_nodes_from(characters)\n",
    "\n",
    "    # Ajouter les arêtes (relations) avec les distances comme attributs\n",
    "    for (char1, char2), distance in distance_dict.items():\n",
    "        G.add_edge(char1, char2, distance=distance)\n",
    "\n",
    "    # Enregistrer le graphe au format GEXF\n",
    "    nx.write_gexf(G, f\"../fig/{file_name}.gexf\")\n",
    "\n",
    "    # Enregistrer les entités nommées dans un fichier CSV\n",
    "    csv_file_path = f\"../fig/paf/{file_name}_PER.csv\"\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Person'])  # Header\n",
    "        csv_writer.writerows([[char] for char in characters])\n",
    "\n",
    "    return csv_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519506f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(18):\n",
    "    file_path = f\"../data/corpus_ASIMOV_leaderboard/corpus_asimov_leaderboard/les_cavernes_d_acier/chapter_{i+1}.txt.preprocessed\"\n",
    "    globals()[f\"chap_{i+1}\"] = file_path\n",
    "    ner_graph_lca(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e88c09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(19):\n",
    "    file_path = f\"../data/corpus_ASIMOV_leaderboard/corpus_asimov_leaderboard/prelude_a_fondation/chapter_{i+1}.txt.preprocessed\"\n",
    "    globals()[f\"chap_{i+1}\"] = file_path\n",
    "    ner_graph_paf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425c225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
