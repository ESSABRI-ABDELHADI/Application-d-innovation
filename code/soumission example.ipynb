{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0020418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac38d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1762b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == \"PER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3689a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour créer le graphe à partir d'un texte\n",
    "def create_graph(text):\n",
    "    G = nx.Graph()\n",
    "    entities = extract_entities(text)\n",
    "\n",
    "    for i in range(len(entities)):\n",
    "        for j in range(i + 1, len(entities)):\n",
    "            # Calculer le nombre de mots entre les entités, limité à 15\n",
    "            words_between = min(15, abs(entities.index(entities[j]) - entities.index(entities[i])) - 1)\n",
    "            if words_between > 0:\n",
    "                # Ajouter l'arête entre les entités avec le poids (nombre de mots entre elles)\n",
    "                G.add_edge(entities[i], entities[j], weight=words_between)\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37fc8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste de chapitres avec les codes de livre associés\n",
    "books = [\n",
    "    (list(range(1, 20)), \"paf\"),\n",
    "    (list(range(1, 19)), \"lca\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc439e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le dictionnaire pour le DataFrame\n",
    "df_dict = {\"ID\": [], \"graphml\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b69f1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins vers les dossiers contenant les chapitres\n",
    "folder_paths = {\n",
    "    \"paf\": \"../data/corpus_ASIMOV_leaderboard/corpus_asimov_leaderboard/prelude_a_fondation/\",\n",
    "    \"lca\": \"../data/corpus_ASIMOV_leaderboard/corpus_asimov_leaderboard/les_cavernes_d_acier/\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d80641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le dictionnaire pour le DataFrame\n",
    "df_dict = {\"ID\": [], \"graphml\": []}\n",
    "\n",
    "# Boucle à travers les livres et les chapitres\n",
    "for chapters, book_code in books:\n",
    "    for chapter in chapters:\n",
    "        # Lire le texte du chapitre à partir du fichier\n",
    "        file_path = os.path.join(folder_paths[book_code], f\"chapter_{chapter}.txt.preprocessed\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            chapter_text = file.read()\n",
    "\n",
    "        # Créer le graphe à partir du texte du chapitre\n",
    "        G = create_graph(chapter_text)\n",
    "\n",
    "        # Ajouter les attributs \"names\" aux nœuds\n",
    "        for node in G.nodes:\n",
    "            G.nodes[node][\"names\"] = node\n",
    "\n",
    "        # Ajouter les données au dictionnaire\n",
    "        df_dict[\"ID\"].append(f\"{book_code}{chapter}\")\n",
    "        df_dict[\"graphml\"].append(\"\".join(nx.generate_graphml(G)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5564dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le DataFrame\n",
    "df = pd.DataFrame(df_dict)\n",
    "df[\"graphml\"] = df[\"graphml\"].apply(lambda x: f'\"{x}\"') \n",
    "\n",
    "# Exporter le DataFrame vers un fichier CSV\n",
    "df.to_csv(\"submission_Niang_Essabri.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec88e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Charger le modèle de langue SpaCy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Fonction pour extraire les entités nommées (personnages) d'un texte\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == \"PER\"]\n",
    "\n",
    "# Fonction pour créer le graphe à partir d'un texte\n",
    "def create_graph(text):\n",
    "    G = nx.Graph()\n",
    "    entities = extract_entities(text)\n",
    "\n",
    "    for i in range(len(entities)):\n",
    "        for j in range(i + 1, len(entities)):\n",
    "            # Calculer le nombre de mots entre les entités, limité à 15\n",
    "            words_between = min(15, abs(entities.index(entities[j]) - entities.index(entities[i])) - 1)\n",
    "            if words_between > 0:\n",
    "                # Ajouter l'arête entre les entités avec le poids (nombre de mots entre elles)\n",
    "                G.add_edge(entities[i], entities[j], weight=words_between)\n",
    "\n",
    "    return G\n",
    "\n",
    "# Chemins vers les dossiers contenant les chapitres\n",
    "folder_paths = {\n",
    "    \"paf\": \"../data/corpus_ASIMOV_leaderboard/corpus_asimov_leaderboard/prelude_a_fondation/\",\n",
    "    \"lca\": \"../data/corpus_ASIMOV_leaderboard/corpus_asimov_leaderboard/les_cavernes_d_acier/\",\n",
    "}\n",
    "\n",
    "# Liste de chapitres avec les codes de livre associés\n",
    "books = [\n",
    "    (list(range(1, 20)), \"paf\"),  # Ajustement pour commencer à 0 et inclure le chapitre 18 pour \"paf\"\n",
    "    (list(range(1, 19)), \"lca\"),  # Ajustement pour commencer à 0 et inclure le chapitre 17 pour \"lca\"\n",
    "]\n",
    "\n",
    "# Initialiser le dictionnaire pour le DataFrame\n",
    "df_dict = {\"ID\": [], \"graphml\": []}\n",
    "\n",
    "# Boucle à travers les livres et les chapitres\n",
    "for chapters, book_code in books:\n",
    "    for chapter in chapters:\n",
    "        # Lire le texte du chapitre à partir du fichier\n",
    "        file_path = os.path.join(folder_paths[book_code], f\"chapter_{chapter}.txt.preprocessed\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            chapter_text = file.read()\n",
    "\n",
    "        # Créer le graphe à partir du texte du chapitre\n",
    "        G = create_graph(chapter_text)\n",
    "\n",
    "        # Ajouter les attributs \"names\" aux nœuds\n",
    "        for node in G.nodes:\n",
    "            G.nodes[node][\"names\"] = node\n",
    "\n",
    "        # Ajouter les données au dictionnaire\n",
    "        graphml_content = \"\".join(nx.generate_graphml(G))\n",
    "        df_dict[\"ID\"].append(f\"{book_code}{chapter}\")\n",
    "        df_dict[\"graphml\"].append(f'\"{graphml_content}\"')  # Encadrer les valeurs de graphml avec des guillemets\n",
    "\n",
    "# Créer le DataFrame\n",
    "df = pd.DataFrame(df_dict)\n",
    "\n",
    "# Exporter le DataFrame vers un fichier CSV\n",
    "df.to_csv(\"submission_Niang_Essabri.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d80aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
